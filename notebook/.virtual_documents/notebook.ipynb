


import pandas as pd
import numpy as np 
import seaborn as sns
import matplotlib.pyplot  as plt

import warnings
warnings.filterwarnings('ignore')

from wordcloud import WordCloud
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix



# path=r
df = pd.read_csv('../data/data.csv', index_col=0)
df.head()


df.info()


df.isnull().sum()


df.dropna(inplace = True)


df.describe()





statuses = df['status'].unique()
statuses


sns.countplot(data=df, x='status', palette='viridis')
plt.title('Count of Mental Health Statuses', fontsize=14)
plt.xlabel('Status', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)



df['statement_word_len'] = df['statement'].apply(lambda x: len(x.split(' ')))
df.head()


df['statement_length'] = df['statement'].apply(lambda x: len(str(x)))

sns.boxplot(data=df, x='status', y='statement_length', palette='Set2')
plt.title('Boxplot of Statement Lengths by Status', fontsize=14)
plt.xlabel('Status', fontsize=12)
plt.ylabel('Statement Length', fontsize=12)
plt.xticks(rotation=45)
plt.show()





# !pip install wordcloud


from wordcloud import WordCloud
text = " ".join(df['statement'].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)

# plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
plt.title('Word Cloud of Statements', fontsize=16)
plt.show()


df


dfgrid=df.copy()





# Cleaning the corpus
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer

def clean_text(text):
    
    text = str(text).lower() # Convert to string and lowercase
    
    text = re.sub(r'\[.*?\]', '', text) # Remove text in square brackets
    
    text = re.sub(r'https?://\S+|www\.\S+|\[.*?\]\(.*?\)', '', text) # Remove URLs
      
    text = re.sub(r'<.*?>+', '', text)# Remove tags
    
    text = re.sub(r'@\w+', '', text)# Remove @
    
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation
    
    text = re.sub(r'\n', ' ', text) # Remove newline
      
    text = re.sub(r'\w*\d\w*', '', text)# Remove words with numbers
      
    text = re.sub(r'\s+', ' ', text)# Remove spaces
    
    return text.strip()


df1 = df.copy()

df1['statement_clean'] = df1['statement'].apply(clean_text)
df1.head()


stop_words = stopwords.words('english') 
# more_stopwords = ['u', 'im', 'c']
# stop_words = stop_words + more_stopwords

def remove_stopwords(text):
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    return text
    
df1['stop_word_statement'] = df1['statement_clean'].apply(remove_stopwords)
df1.head()


 
# stemmer = nltk.PorterStemmer()

def stemm_text(text):
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    return text

df1['stemmer_statement'] = df1['statement_clean'].apply(stemm_text) 
df1.head()


nltk.download('punkt')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split(' '))
    return text

df1['lemmatizer_statement'] = df1['statement_clean'].apply(lemmatize_text)
df1.head()


#function

def preprocess_data(text):
    #Clean test
    text = clean_text(text)
    # Remove stopwords
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    # Stemm all the words in the sentence
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    
    return text

df1['preprocess_data'] = df1['statement'].apply(preprocess_data)
df1.head()


from sklearn.preprocessing import LabelEncoder

df2=df1.copy()

encoder = LabelEncoder()
df2['status_encoded'] = encoder.fit_transform(df2['status']) 
df2.head()


from sklearn.model_selection import train_test_split

X = df2['preprocess_data']  # Feature: Text data
y = df2['status']  # Target: Encoded labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Sample data
documents = ["I love machine learning", "I love deep learning"]

# Using CountVectorizer
count_vectorizer = CountVectorizer()
count_vectors = count_vectorizer.fit_transform(documents)
print("CountVectorizer Results:")
print(count_vectors.toarray())  # Display word count matrix

# Using TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()
tfidf_vectors = tfidf_vectorizer.fit_transform(documents)
print("\nTfidfVectorizer Results:")
print(tfidf_vectors.toarray())  # Display tf-idf matrix





count_vector = CountVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.7, max_features=5000)
# count_vector.fit(X_train)

X_train_cv = count_vector.fit_transform(X_train)
X_test_cv = count_vector.transform(X_test)

tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

#using count_vector
logr = LogisticRegression(random_state= 42) 

# Train the classifier
logr.fit(X_train_cv, y_train)
y_pred = logr.predict(X_test_cv)
labels = encoder.classes_
acc = accuracy_score(y_test, y_pred)
print('Accuracy: ',acc)
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Logistic Regression')
plt.show()


# using tfidf
logr = LogisticRegression(random_state= 42) 

# Train the classifier
logr.fit(X_train_tfidf, y_train)
y_pred = logr.predict(X_test_tfidf)
labels = encoder.classes_
acc = accuracy_score(y_test, y_pred)
print('Accuracy: ',acc)
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Logistic Regression')
plt.show()


from imblearn.over_sampling import SMOTE, RandomOverSampler

sm = SMOTE() 
X_train_sm, y_train_sm = sm.fit_resample(X_train_tfidf, y_train)
print((X_train_sm.shape), (y_train_sm.shape))


# using tfidf with smote
logr = LogisticRegression(random_state= 42) 

# Train the classifier
logr.fit(X_train_sm, y_train_sm)
y_pred = logr.predict(X_test_tfidf)
labels = encoder.classes_
acc = accuracy_score(y_test, y_pred)
print('Accuracy: ',acc)
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Logistic Regression')
plt.show()


print(X_train_tfidf.shape,X_test_tfidf.shape, y_train.shape,y_test.shape)



# !pip install lightgbm
# !pip install catboost


from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

models = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    # "Random Forest": RandomForestClassifier(),
    # "SVM": SVC(probability=True),
    # "GBC": GradientBoostingClassifier(),
    # "Tree":DecisionTreeClassifier(),
    # "XGBoost": XGBClassifier(use_label_encoder=True, eval_metric='logloss'),
    # "KNN":KNeighborsClassifier(),
    "LightGBM": LGBMClassifier(),
}


results = {}
for name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    print("=" * 20)


results_df = pd.DataFrame(results.items(), columns=['Model', 'Accuracy']).sort_values(by='Accuracy', ascending=False)
results_df


dfgrid


import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
stemmer = nltk.SnowballStemmer("english")
stop_words = stopwords.words('english')
nltk.download('punkt')
nltk.download('wordnet')

def helper_text(text):
    text = str(text).lower()  # Convert to string and lowercase
    
    text = re.sub(r'\[.*?\]', '', text)  # Remove text in square brackets
    
    text = re.sub(r'https?://\S+|www\.\S+|\[.*?\]\(.*?\)', '', text)  # Remove URLs
    
    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags
    
    text = re.sub(r'@\w+', '', text)  # Remove @ mentions
    
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    
    text = re.sub(r'\n', ' ', text)  # Remove newlines
    
    text = re.sub(r'\w*\d\w*', '', text)  # Remove words with numbers
    
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    
    text = text.strip()  # Remove leading and trailing spaces
    
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)
    
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    
    return text

dfgrid['clean'] = dfgrid['statement'].apply(helper_text)
dfgrid.head()


from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Encoding target variable
encoder = LabelEncoder()
dfgrid['status_encoded'] = encoder.fit_transform(dfgrid['status']) 

# Features and target
X = dfgrid['clean']  # Text data (preprocessed)
y = dfgrid['status_encoded']  # Encoded target variable

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create the pipeline with TF-IDF vectorizer and classifier
gaus = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000, stop_words='english')),  # TF-IDF vectorization
    ('classifier', GaussianNB()),  # Gaussian Naive Bayes
])

# Train the model (convert sparse matrix to dense by using .toarray())
X_train_dense = gaus.named_steps['vectorizer'].fit_transform(X_train).toarray()
gaus.named_steps['classifier'].fit(X_train_dense, y_train)

# Transform test data and convert to dense format
X_test_dense = gaus.named_steps['vectorizer'].transform(X_test).toarray()

# Make predictions
y_pred = gaus.named_steps['classifier'].predict(X_test_dense)

# Evaluate the model
acc = accuracy_score(y_test, y_pred)
print(f'Accuracy: {acc:.4f}')

# Confusion matrix and classification report
labels = encoder.classes_  # Use the original labels from encoder
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

# Plot confusion matrix as a heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Classifier')
plt.show()



mul = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000, stop_words='english')),  # TF-IDF vectorization
    ('classifier', MultinomialNB()),
])

# Train the model
mul.fit(X_train, y_train)

# Make predictions
y_pred = mul.predict(X_test)

# Evaluate the model
acc = accuracy_score(y_test, y_pred)
print(f'Accuracy: {acc:.4f}')

# Confusion matrix and classification report
labels = encoder.classes_  # Use the original labels from encoder
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

# Plot confusion matrix as a heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Classifier')
plt.show()


bern = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000, stop_words='english')),  # TF-IDF vectorization
    ('classifier', BernoulliNB()),
])

# Train the model
bern.fit(X_train, y_train)

# Make predictions
y_pred = bern.predict(X_test)

# Evaluate the model
acc = accuracy_score(y_test, y_pred)
print(f'Accuracy: {acc:.4f}')

# Confusion matrix and classification report
labels = encoder.classes_  # Use the original labels from encoder
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

# Plot confusion matrix as a heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Classifier')
plt.show()


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

# Data preparation: Encoding the target variable
encoder = LabelEncoder()
dfgrid['status_encoded'] = encoder.fit_transform(dfgrid['status']) 

# Features and target
X = dfgrid['clean']  # Text data (preprocessed)
y = dfgrid['status_encoded']  # Encoded target variable

# Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create the pipeline with TF-IDF vectorizer and classifier
pipe = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000, stop_words='english')),  # TF-IDF vectorization
    ('classifier', LGBMClassifier()),
])

# Train the model
pipe.fit(X_train, y_train)

# Make predictions
y_pred = pipe.predict(X_test)

# Evaluate the model
acc = accuracy_score(y_test, y_pred)
print(f'Accuracy: {acc:.4f}')

# Confusion matrix and classification report
labels = encoder.classes_  # Use the original labels from encoder
conf_matrix = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, target_names=labels))

# Plot confusion matrix as a heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for LGBM Classifier')
plt.show()

# model_dict = {'model': pipe, 'labels': encoder}
# joblib.dump(model_dict, 'pipe_model.pkl')

joblib.dump({'model':pipe,'labels':encoder}, 'pipe_model.pkl')


# from sklearn.model_selection import GridSearchCV, train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.pipeline import Pipeline
# from sklearn.metrics import accuracy_score
# x = dfgrid['clean'] 
# y = dfgrid['status'] 
# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)


# pipeline = Pipeline([
#     ('vectorizer', TfidfVectorizer(max_features=5000, stop_words='english')),  # Text vectorization
#     ('classifier', LGBMClassifier())  # LightGBM classifier
# ])

# Define the parameter grid for GridSearchCV to tune LightGBM's hyperparameters
# param_grid = {
#     'classifier__num_leaves': [31, 50, 100],  # Number of leaves in the tree
#     'classifier__max_depth': [-1, 10, 20],  # Maximum depth of trees (-1 means no limit)
#     'classifier__learning_rate': [0.05, 0.1, 0.2],  # Learning rate for boosting
#     'classifier__n_estimators': [20, 40, 100],  # Number of boosting iterations (trees)
#     'classifier__subsample': [0.7, 0.8, 1.0],  # Fraction of samples used for each tree
#     'classifier__colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features used for each tree
# }


# grid_search = GridSearchCV(pipeline, param_grid, cv=3, verbose=1, n_jobs=-1)

# grid_search.fit(X_train, y_train)


# print(f"Best parameters found: {grid_search.best_params_}")
# print(f"Best cross-validation accuracy: {grid_search.best_score_}")

# Use the best model found from GridSearchCV on the test set
# best_model = grid_search.best_estimator_
# y_pred = best_model.predict(X_test)

# Evaluate the model's performance on the test set
# accuracy = accuracy_score(y_test, y_pred)
# print(f"Test accuracy: {accuracy}")









import joblib
import re
import string
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from lightgbm import LGBMClassifier

# Initialize stopwords and stemmer
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

# Function to clean the input text
def clean_text(text):
    """Cleans the input text by removing unnecessary characters, stopwords, and applying stemming."""
    # Convert to lowercase and string
    text = str(text).lower()
    
    # Remove unwanted characters (URLs, square brackets, HTML tags, mentions, punctuation, etc.)
    text = re.sub(r'\[.*?\]', '', text)  # Remove text in square brackets
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags
    text = re.sub(r'@\w+', '', text)  # Remove @ mentions
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)  # Remove punctuation
    text = re.sub(r'\n', ' ', text)  # Remove newlines
    text = re.sub(r'\w*\d\w*', '', text)  # Remove words containing digits
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    text = text.strip()  # Remove leading and trailing spaces
    
    # Remove stopwords and apply stemming
    text = ' '.join(word for word in text.split(' ') if word not in stop_words)  # Remove stopwords
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))  # Apply stemming
    
    return text

# Sample text input from the user
test_input = input("Enter text: ") 

# Clean the input text
cleaned_text = clean_text(test_input)

# Load the pre-trained model
loaded_pkl = joblib.load("pipe_model.pkl")
model = loaded_pkl['model']
label= loaded_pkl['labels']

prediction = model.predict([cleaned_text])

print(f"prediction: {prediction[0]}")

predicted_label = label.inverse_transform(prediction)

print(f"Predicted label: {predicted_label[0]}")



print(df['statement'].iloc[1000])
df['status'].iloc[1000]



